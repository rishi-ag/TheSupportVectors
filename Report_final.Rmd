---
title: "Final Report_Support Vectors"
author: "Anestis Papanikolaou, Rishabh Agnihotri, Joan Verdú"
date: "Friday, March 27, 2015"
output: html_document
---



```{r,setting up the environment, echo = FALSE, include=FALSE}
setwd("C:/Users/Anestis/Documents/GitHub/TheSupportVectors")
if (!require("knitr")) install.packages("knitr")
if (!require("rmarkdown")) install.packages("rmarkdown")

opts_chunk$set(fig.width=8, fig.height=5, warning=FALSE, message=FALSE, 
               include=TRUE, echo=TRUE, cache=TRUE, cache.comments=FALSE)

if(!require("reshape2"))install.packages("reshape2")
if(!require("ggplot2"))install.packages("ggplot2")
if(!require("ggthemes"))install.packages("ggthemes")


source("code/library.R")
train <- get.train.data()
train.label <- train[[1]]
labels_df <- as.data.frame(train.label)
train.feat <- train[[3]]
```


#Introduction
This report summarizes the statistical modeling and analysis of the Kaggle Competition for the prediction of the Forest covertype, a project which constitutes part of the Machine Learning class of Master of Data Science, BGSE. The purpose of the report
is to document both the implemented algorithms and data analysis perfomed together with the corresponding data modeling and inference techniques used during the subsequent statistical analyses. 

# 1. Data Preprocessing

We first separated the labels from the features. 

Two standardization procedures were conducted for the continuous variables (1-10):

1. Centred the data around the mean and divided by the standard deviation.
2. Rescale the data from -1 to 1.

We now have three versions of features, both for test and training features
1. Raw - No changes to features 
2. Standardised - Continuius variables are centered around sample mean and divided by sample standard deviation.
3. Scaled - Continuous variables are scaled from -1 to 1

# Principal Componenent Analysis (PCA)
PCA was also used as part of the Data preprocessing and exploratory analysis in order to 

Principal Component Analysis (PCA) is implemented in order to check whether there are redundant features which do not contribute to the variability of the data and thus achieve dimensionality reduction of our data. The steps followed for the implementation of the PCA were the following:

1. Standardize the values of the features - a necessary procedure for conducting PCA
2. PCA and binary data: The literature is quite ambiguous about this topic, although it seems like the specific technique is appropriate for handling binary (categorical) datacombined with other (continuous valued) features. when standardized.
3. Apply the prcomp function which conducts the calculation by taking the singular value decomposition of the covariance matrix of the features.
4. Retain a ~99% level of  variability of our initial data.
5. Use the Random Forest method on the reduced data.

The results of the PCA combined with the Random Forest algorithm were not improved and 

```{r, echo=FALSE}

pca<-prcomp(train.feat)
train.feat<-pca$x
vectors<-pca$rotation
var<-diag(cov(train.feat))/sum(diag(cov(train.feat)))
cumvar<-cumsum(var)
barplot(cumvar)
nvars<-sum(cumvar<0.99)
sum(cumvar<0.95)
# Get only the 99% variance variables from PCA variables
train.feat<-train.feat[,1:nvars]
vectors<-vectors[,1:nvars]
```



# 2. Modelling

There were different models tried and the parameters were optimized for each method using the AWS. More specificcally:

<b> 1. K-NN model:</b> 

We implemented the K-NN algorithm for all the odd cases between k=1-21 by using both the standardized and the raw data in order to check which formulation gives the higher accuracy. In the end it turned out that the 1-NN method provides the best results, as it is depicted in the figures below. 


```{r, echo=FALSE}
setwd("C:/Users/Anestis/Documents/GitHub/TheSupportVectors")
comparison <- read.csv(file = "data/knn/knn_data_error.csv", header = T)[,-1]


#melt daa for graph
comparison.long <- melt(data = comparison, id.vars = "k", 
                        measure.vars = c("raw.feat.err", "std.feat.err"),
                        variable.name = "feature_type", value.name = "error")

plot1 <- ggplot(data = comparison.long, aes(x = k,y = error, color = feature_type )) +
  geom_line(size = 0.5) + 
  geom_point(size = 3) + 
  scale_x_continuous(breaks= seq(1,21,2)) + 
  ggtitle("k error (raw vs std features)") +
  scale_color_wsj(name  ="Feature Type", labels=c("Raw", "Standardised"))
plot1
```


<b> 2. Kernalised Support Vector Machine model:</b>

We also tried radial kernalised support vector machines. The challange was to optimise
the cost parameter and the gamma(for the kernal) parameter.

Features used - rescaled

We performed a grid search over some combinations of cost and gamma and get the 
following results

```{r,echo = FALSE}
setwd("C:/Users/Anestis/Documents/GitHub/TheSupportVectors")
res <- read.csv(file = "data/svm/coarse_grid_errors.csv", header = T)
res
```

Although the results are cross validated for k = 10, we cannot help but suspect that
there might be overfitting involved. Indeed testing parameters that gave zero error returned comparitively high errors on the test data.

To further tune our parameters, we chose the gamma and cost corresponding to the eighth row and tried to optimise the paramerets further around `cost = 512` and
`gamma = .5`, parameters that gave us the best test error.

```{r,echo = FALSE}
setwd("C:/Users/Anestis/Documents/GitHub/TheSupportVectors")
fig <- read.csv(file = "data/svm/fine_grid_errors.csv")[,-1]
fig

```

However, despite the encouraging training errors, we could only manage a best
test error of around 15%, no less than we got from the broad grid search.

