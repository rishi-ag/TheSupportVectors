j<-iter %%length(data$Y)+1
if (verbose==T) {cat('\niter: ',iter,' coefs: ',as.character(coefs))}
if (data$Y[j]!=sign(sum(coefs*data[j,cdata]))){ #classification error
coefs<-coefs + data$Y[j]*data[j,cdata] #Actualize coefficients
last<-j #Actualize indicator of last change
} else {
if (last==j) {
break # break if one complete round without changes
}
}
}
return(list(coefs=coefs,iter=iter))
}
#' plot.perceptron function
#'
#' Executes bivariate variable perceptron estimates of coefficients to classify a binary class
#'
#' @param data Data frame, first two columns are x2, x1, third column is a two level class, fouth column its +1 -1 equivalent
#' @param coefs Estimade coefficients for the perceptron (x2 coef, x1 coef, b coef)
#' @param minpoint Data frame of single point, just as 'data' but adding Distance column with the desired word to name it
#' @param plotmin Boolean. True if needed to plot the minpoint
#' @return plot of data, classification line, and minimum distance point
#' @export
#'
plot.perceptron<-function(data,coefs,minpoint=NULL,plotmin=F){
line<-data.frame(slope=0,int=0)
names(data)[1:3]<-c("weight","height","class")
#calculate slope and intercept
line$slope<-as.numeric(-(coefs[2]/coefs[1]))
line$int<-as.numeric(-(coefs[3]/coefs[1]))
if (plotmin==T){ #plot including min distance classifier
plot<- ggplot() +
geom_point(data = data, aes(x = height, y = weight,
colour=class, fill=class)) +
geom_point(data=minpoint,aes(x=height,y=weight,shape=Distance))+
xlab("X1") +
ylab("X2") +
geom_abline(data=line,aes(slope=slope,
intercept=int))
} else {
#Plot data and classifiers
plot<-ggplot(data = data, aes(x = height, y = weight,
colour=class, fill=class)) +
geom_point() +
xlab("X1") +
ylab("X2") +
geom_abline(data=line,aes(slope=slope,
intercept=int))
}
return(plot)
}
#' classify perceptron function
#'
#' Classifies using bivariate variable perceptron estimates of coefficients to classify a binary class, and computes final distance to threshold.
#'
#' @param data Data frame, first two columns are x2, x1, third column is a two level class, fouth column its +1 -1 equivalent
#' @param coefs Initial coefficients for the perceptron (x2 coef, x1 coef, b coef)
#' @return Data frame with predictions and distances
#' @export
classify.perceptron<-function(data,coefs){
data$i<-rep(1,length(data$Y))
cdata<-c(1,2,5)
pred<-rep(NA,dim(data)[1])
dist<-rep(NA,dim(data)[1])
int<-as.numeric(-(coefs[3]/coefs[1])) #intercept
# Classifiy and distance
for(i in 1:length(data$Y)){
pred[i]<-sign(sum(coefs*data[i,cdata]))
dist[i]<-abs((coefs[1]*(data[i,1]-int)+coefs[2]*data[i,2])
/sqrt(sum(coefs[1:2]^2)))
}
dist<-unlist(dist)
return(data.frame(pred=pred,dist=dist))
}
#' Radius of data function
#'
#' Calculates max radius of bivariatedata as distance to its mean
#'
#' @param data Data frame, first two columns are x2, x1
#' @return Numeric maximum distance from a point to the centroid of data
#' @export
calc.radius<-function(data){
radius<-0
mean<-colMeans(data[,1:2])
for (i in 1:dim(data)[1]){
dist<-sum((mean-data[i,1:2])^2)
if (dist>radius) {radius<-dist}
}
return(sqrt(radius))
}
#' Optimal margin function
#'
#' Calculates optimal margin separation of bivariatedata as distance to linear svm classifier line
#'
#' @param data Data frame, first two columns are x2, x1,class is Y (-1,+1)
#' @return Numeric optimal classification margin
#' @export
calc.gamma<-function(data){
names(data)[1:2]<-c("weight","height")
gamma<-0
svm.model <- svm(Y ~ height+weight, data=data, type='C-classification',
kernel='linear',scale=FALSE)
border<-data[svm.model$index,]
w <- t(svm.model$coefs) %*% svm.model$SV
b <- -svm.model$rho
p <- svm.model$SV
intp<--b/w[1,2]
sl<--w[1,1]/w[1,2]
svm.coefs<-c(1,-sl,-intp)
names(svm.coefs)<-c("weight","height","i")
norm.w<-sqrt(svm.coefs[1]^2+svm.coefs[2]^2)
gamma<-sum(svm.coefs[1:2]*(border[1,1:2]-c(intp,0)))/norm.w
#check wih other point uniqueness
# gamma<-sum(svm.coefs[1:2]*border[2,1:2])/norm.w
return(as.numeric(abs(gamma)))
}
#-------------------------
#### Algorithm ###########
#-------------------------
#Data generation case separation
data<-data.gen(number=100,overlap=F)
coefs<-perceptron(data,50000)$coefs
plot<-plot.perceptron(data,coefs)
plot
out<-classify.perceptron(data,coefs)
head(out)
# Plot max margin solution with linear support vector model
svm.model <- svm(Y ~ height+weight, data=data, type='C-classification',
kernel='linear',scale=FALSE)
border<-data[svm.model$index,]
border$Distance<-c("svm")
w <- t(svm.model$coefs) %*% svm.model$SV
b <- -svm.model$rho
p <- svm.model$SV
intp<--b/w[1,2]
sl<--w[1,1]/w[1,2]
svm.coefs<-c(1,-sl,-intp)
# plot perceptron and linear svm lines
plot+geom_abline(slope=sl,intercept=intp,colour="red")
#Errors
Errors<-(length(data$Y)-sum(out$pred*data$Y))/2
Errors/lenght(data$Y)#percent errors
# Plot with min dist point
minpoint<-data[which.min(out$dist),]
minpoint$Distance<-c('minimum')
plot.perceptron(data,coefs,minpoint,plotmin=T)
#### Case overlap data
data<-data.gen(number=100,overlap=T)
coefs<-perceptron(data,20000)$coefs
plot<-plot.perceptron(data,coefs)
plot
out<-classify.perceptron(data,coefs)
#Errors
Errors<-(length(data$Y)-sum(out$pred*data$Y))/2
Errors/length(data$Y) #percent errors
# Plot with min dist point
minpoint<-data[which.min(out$dist),]
minpoint$Distance<-c('minimum')
plot.perceptron(data,coefs,minpoint,plotmin=T)
# Check efect of radius and minimum distance on number of iterations
# Generate random data and check
data<-data.gen(number=100,overlap=F)
#including points in sequence along centers of data,
# affects distance and radius at same time
c1center<-c(3, 150)
c2center<-c(c(10, 80))
mean<-colMeans(data[,1:2])
slpct<-(c1center[1]-c2center[1])/(c1center[2]-c2center[2])
intct<-c1center[1]-c1center[2]*slpct
#seq of points from center of class 2 of data to center of data,
#generate each time closer points
n_each<-5
intpoint<-seq(80,mean[2],length.out=n_each)
intpoint<-as.data.frame(cbind(intpoint*slpct+intct,intpoint))
intpoint<-intpoint[1:(n_each-1),] #remove point at mean
names(intpoint)<-c("weight","height")
# idem but outer
outpoint<- seq(0,80,length.out=n_each)
outpoint<-as.data.frame(cbind(outpoint*slpct+intct,outpoint))
outpoint<-outpoint[1:(n_each-1),] #remove last point repeated
names(outpoint)<-c("weight","height")
#join
points<-rbind(outpoint,intpoint)
#plot to check it
ggplot() +
geom_point(data = data, aes(x = height, y = weight,
colour=Animal, fill=Animal)) +
geom_point(data=points,aes(x=height,y=weight))
# for each new center of class 1, play with increasing or decreasin SD in order
# to generate data with diferent margin of classification and radius
n_sd<-5
n_xclass<-100
max_iter<-30000
sd<-data.frame(sdx2=seq(1,6,length.out=n_sd),sdx1=seq(4,20,length.out=n_sd))
results<-matrix(NA,dim(points)[1]*dim(sd)[1],ncol=5)
for(i in (1:dim(points)[1])){
for (j in (1:dim(sd)[1])){
cat('\n Point ',i,' SD ',j)
data <- data.gen(n_xclass,meanc2= as.numeric(points[i,]),
sdc2=as.numeric(sd[j,]),overlap=F)
percp<-perceptron(data,max_iter)
# Radius and gamma calculations
gamma<-calc.gamma(data)
radius<-calc.radius(data)
results[j+(i-1)*dim(sd)[1],]<-c(i,j,radius,gamma,percp$iter)
}
}
# Final plot
results<-as.data.frame(results)
names(results)<-c("center2","sd2","radius","gamma","iterations")
results<-results[results$iterations<max_iter,]
results$rad_dist<-(results$radius/results$gamma)^2
ggplot(results, aes(x=radius, y=iterations)) +
geom_point(shape=1) +    # Use hollow circles
geom_smooth(method=lm)+
ggtitle("Performance of percerptron algorithm")
ggplot(results, aes(x=gamma, y=iterations)) +
geom_point(shape=1) +    # Use hollow circles
geom_smooth(method=lm)+
ggtitle("Performance of percerptron algorithm")
ggplot(results, aes(x=rad_dist, y=iterations)) +
geom_point(shape=1) +    # Use hollow circles
geom_smooth(method=lm)+
xlab(expression(paste((radius/gamma)^bold("2"))))+
ggtitle("Performance of percerptron algorithm")
data<-data.frame(rnorm(10),rnorm(10))
calc.radius(data)
plot(data)
plot(data,asp=1)
mean<-colMeans(data[,1:2])
mean
results
library(tree)
library(tree)
install.packages("tree")
install.packages("rpart")
?rpart
library(rpart)
library(tree)
ir.tr <- tree(Species ~., iris)
ir.tr
summary(ir.tr)
plot(ir.tr)
fit <- tree(Species ~., iris)
fit
summary(fit)
plot(fit, uniform=TRUE, main="Classification Tree")
text(fit, use.n=TRUE, all=TRUE, cex=.8)
plot(fit, main="Classification Tree")
text(fit,  all=TRUE, cex=.8)
str(iris)
data<-iris
fit2 <- rpart(data$Species ~ ., method="class")
data<-iris
fit2 <- rpart(Species ~ Sepal.Length+Sepal.Width+Petal.Length+Petal.Width,
data=data, method="class")
plot(fit, uniform=TRUE, main="Classification Tree")
text(fit, use.n=TRUE, all=TRUE, cex=.8)
rpart.control(maxdepth=1)
fit2 <- rpart(Species ~ Sepal.Length+Sepal.Width+Petal.Length+Petal.Width,
data=data, method="class")
plot(fit, uniform=TRUE, main="Classification Tree")
text(fit, use.n=TRUE, all=TRUE, cex=.8)
control<-rpart.control(maxdepth=1)
fit2 <- rpart(Species ~ Sepal.Length+Sepal.Width+Petal.Length+Petal.Width,
data=data, control=control,method="class")
plot(fit, uniform=TRUE, main="Classification Tree")
text(fit, use.n=TRUE, all=TRUE, cex=.8)
control<-rpart.control(maxdepth=1)
fit2 <- rpart(Species ~ Sepal.Length+Sepal.Width+Petal.Length+Petal.Width,
data=data, control=rpart.control(maxdepth=1),
params=list(split="gini"),method="class")
plot(fit2, uniform=TRUE, main="Classification Tree")
text(fit2, use.n=TRUE, all=TRUE, cex=.8)
fit2 <- rpart(Species ~ Sepal.Length+Sepal.Width+Petal.Length+Petal.Width,
data=data, control=control,
method="class")
plot(fit2, uniform=TRUE, main="Classification Tree")
text(fit2, use.n=TRUE, all=TRUE, cex=.8)
str(data)
out<-predict(fit2,data[,-5])
head(out)
head(out,50)
tail(out,50)
tail(out,100)
out<-predict(fit2,data[,-5],type="class")
tail(out,100)
ntrees<-c(1:(dim(data)[1]))
devtools::install_github("rstudio/rmarkdown")
rmarkdown::render('in.md',
output_format=pdf_document(latex_engine='xelatex')
)
55/1.5
33*8
if (!require("foreach")) install.packages("foreach")
if (!require("doSNOW")) install.packages("doSNOW")
if (!require("caret")) install.packages("caret")
setwd("D:/master/kaggle/TheSupportVectors")
source("code/library.R")
# funtion to unify related factor data
unify<-function(data,vector,text){
nobs<-dim(data)[1]
out<-rep(0,nobs)
for (j in 1:nobs){
for (i in seq_along(vector)){
out[j]<-paste0(out[j],data[[paste0(text,as.character(vector[i]))]][j])
}
}
return(as.factor(out))
}
#get data for rforest
train.data <- get.train.data()
labels <- as.factor(train.data[[1]])
features <- train.data[[3]] #standarized data
#features[,10:53]<-as.data.frame(apply(features[,10:53],2,as.factor))
test.data <- get.test.data()
test.feat <- test.data[[2]] #standarized data
#test.feat[,10:53]<-as.data.frame(apply(test.feat[,10:53],2,as.factor))
prova<-unify(features,1:40,"soil_type_")
sample<-createDataPartition(y=labels,p=0.01,list=FALSE)#sample preserving proportion of classes
features<-features[sample,]
labels<-labels[sample]
## data partition to select model
#partition preserving proportion of classes
subtrain<-createDataPartition(y=labels,p=0.8,list=FALSE)
trainstrain<-features[subtrain,]
trainstest<-features[-subtrain,]
trainset<-data.frame(labels=labels[subtrain],trainstrain)
testset<-data.frame(labels=labels[-subtrain],trainstest)
# Chech different models
mod_names<-c('Bagged AdaBoost', 'Bagged CART', 'Bagged Flexible Discriminant Analysis','Bagged Logic Regression','Model Averaged Neural Network','Random Forest','Boosted Classification Trees','Boosted Generalized Linear Model','Boosted Logistic Regression','Boosted Tree','Support Vector Machines with Class Weights','High Dimensional Discriminant Analysis','Linear Discriminant Analysis','Oblique Random Forest','Parallel Random Forest','glmnet','Least Squares Support Vector Machine with Polynomial Kernel','Least Squares Support Vector Machine with Radial Basis Function Kernel','Adaptive Mixture Discriminant Analysis','Logistic Model Trees','Neural Network','k-Nearest Neighbors')
mod_code<-c('AdaBag', 'treebag', 'bagFDA','logicBag','avNNet','rf','ada','glmboost','LogitBoost','bstTree','svmRadialWeights','hdda','lda','ORFsvm','parRF','glmnet','lssvmPoly','lssvmRadial','amdai','LMT','nnet','knn')
set.seed(10)
# preparing the parallelization
noCores <- detectCores()-1
cl <- makeCluster(noCores, type="SOCK", outfile="")
registerDoSNOW(cl)
models_outcome<-foreach(mod = 2:4,
.combine=rbind, .packages=c("caret", "dplyr")) %dopar% {
# some helpful debugging messages
cat("The current model is ", mod, "\n")
# model
model<-train(labels~.,data=trainset,method=mod_code[mod])
pred<-predict(model,newdata=trainstest)
res<-confusionMatrix(pred,testset$labels)
result<-c(res$overall,res$byClass)
}
models_outcome
train<-function(trainset,mod_code){
model<-train(labels~.,data=trainset,method=mod_code[mod])
pred<-predict(model,newdata=trainstest)
res<-confusionMatrix(pred,testset$labels)
result<-c(res$overall,res$byClass)
}
if(!require("doParallel"))install.packages("doParallel")
cores <- detectCores()
cl <- makeCluster(cores)
clusterExport(cl, list("trainset", "mod_code",
"caret", "train"), envir = environment())
registerDoParallel(cl)
#call function to compare k error rates on features and standardised features
system.time(perf<- parSapply(cl, mod_code,
function(x) train(trainset, x)))
stopCluster(cl)
trytrain<-function(trainset,mod_code){
model<-train(labels~.,data=trainset,method=mod_code[mod])
pred<-predict(model,newdata=trainstest)
res<-confusionMatrix(pred,testset$labels)
result<-c(res$overall,res$byClass)
}
cores <- detectCores()
cl <- makeCluster(cores)
clusterExport(cl, list("trainset", "mod_code",
"caret", "train","trytrain"), envir = environment())
registerDoParallel(cl)
if(!require("caret"))install.packages("caret")
cores <- detectCores()
cl <- makeCluster(cores)
clusterExport(cl, list("trainset", "mod_code",
"caret", "train","trytrain"), envir = environment())
registerDoParallel(cl)
clusterExport(cl, list("trainset", "mod_code",
"train","trytrain"), envir = environment())
registerDoParallel(cl)
system.time(perf<- parSapply(cl, mod_code,
function(x) train(trainset, x)))
stopCluster(cl)
cores <- detectCores()
cl <- makeCluster(cores)
clusterExport(cl, list("trainset", "mod_code",
"train","trytrain"), envir = environment())
registerDoParallel(cl)
#call function to compare k error rates on features and standardised features
system.time(perf<- parSapply(cl, mod_code,
function(x) trytrain(trainset, x)))
stopCluster(cl)
trytrain<-function(trainset,mod_code){
model<-train(labels~.,data=trainset,method=mod_code)
pred<-predict(model,newdata=trainstest)
res<-confusionMatrix(pred,testset$labels)
result<-c(res$overall,res$byClass)
}
cores <- detectCores()
cl <- makeCluster(cores)
clusterExport(cl, list("trainset", "mod_code",
"train","trytrain"), envir = environment())
registerDoParallel(cl)
#call function to compare k error rates on features and standardised features
system.time(perf<- parSapply(cl, mod_code,
function(x) trytrain(trainset, x)))
stopCluster(cl)
trytrain<-function(trainset,testset,mod_code){
model<-train(labels~.,data=trainset,method=mod_code)
pred<-predict(model,newdata=testset)
res<-confusionMatrix(pred,testset$labels)
result<-c(res$overall,res$byClass)
}
cores <- detectCores()
cl <- makeCluster(cores)
clusterExport(cl, list("trainset", "mod_code",
"train","trytrain","testset"), envir = environment())
registerDoParallel(cl)
#call function to compare k error rates on features and standardised features
system.time(perf<- parSapply(cl, mod_code,
function(x) trytrain(trainset, testset,x)))
stopCluster(cl)
cores <- detectCores()
cl <- makeCluster(cores)
clusterExport(cl, list("trainset", "mod_code",
"train","trytrain","testset"), envir = environment())
registerDoParallel(cl)
#call function to compare k error rates on features and standardised features
system.time(perf<- parSapply(cl, 1:4,
function(x) trytrain(trainset, testset,mod_code[x])))
stopCluster(cl)
trytrain<-function(trainset,testset,mod_code){
library(caret)
model<-train(labels~.,data=trainset,method=mod_code)
pred<-predict(model,newdata=testset)
res<-confusionMatrix(pred,testset$labels)
result<-c(res$overall,res$byClass)
}
cores <- detectCores()
cl <- makeCluster(cores)
clusterExport(cl, list("trainset", "mod_code",
"train","trytrain","testset"), envir = environment())
registerDoParallel(cl)
#call function to compare k error rates on features and standardised features
system.time(perf<- parSapply(cl, 1:4,
function(x) trytrain(trainset, testset,mod_code[x])))
stopCluster(cl)
p<-tryrain(trainset,testset,mod_code[1])
p<-trytrain(trainset,testset,mod_code[1])
model<-train(labels~.,data=trainset,method=mod_code[1])
model<-train(labels~.,data=trainset,method='rf')
?train
head(trainset)
model<-train(trainset[,-1],trainset[,1],method='rf')
model<-train(trainset[,-1],trainset[,1])
rm(ls=())
rm(ls=c())
rm()
rm(ls)
rm(list=c())
rm(list=ls
)
rm train
rm(train)
model<-train(trainset[,-1],trainset[,1],method='rf')
trytrain<-function(trainset,testset,mod_code){
library(caret)
model<-train(labels~.,data=trainset,method=mod_code)
pred<-predict(model,newdata=testset)
res<-confusionMatrix(pred,testset$labels)
result<-c(res$overall,res$byClass)
}
cores <- detectCores()
cl <- makeCluster(cores)
clusterExport(cl, list("trainset", "mod_code",
"train","trytrain","testset"), envir = environment())
registerDoParallel(cl)
#call function to compare k error rates on features and standardised features
system.time(perf<- parSapply(cl, 1:4,
function(x) trytrain(trainset, testset,mod_code[x])))
length(mod_code)
labels <- as.factor(train.data[[1]])
features <- train.data[[3]]
prova<-unify(features,1:40,"soil_type_")
labels <- as.factor(train.data[[1]])
features <- train.data[[3]] #standarized data
sample<-createDataPartition(y=labels,p=0.05,list=FALSE)#sample preserving proportion of classes
features<-features[sample,]
labels<-labels[sample]
## data partition to select model
#partition preserving proportion of classes
subtrain<-createDataPartition(y=labels,p=0.8,list=FALSE)
trainstrain<-features[subtrain,]
trainstest<-features[-subtrain,]
trainset<-data.frame(labels=labels[subtrain],trainstrain)
testset<-data.frame(labels=labels[-subtrain],trainstest)
# Chech different models
mod_names<-c('Bagged AdaBoost', 'Bagged CART', 'Bagged Flexible Discriminant Analysis','Bagged Logic Regression','Model Averaged Neural Network','Random Forest','Boosted Classification Trees','Boosted Generalized Linear Model','Boosted Logistic Regression','Boosted Tree','Support Vector Machines with Class Weights','High Dimensional Discriminant Analysis','Linear Discriminant Analysis','Oblique Random Forest','Parallel Random Forest','glmnet','Least Squares Support Vector Machine with Polynomial Kernel','Least Squares Support Vector Machine with Radial Basis Function Kernel','Adaptive Mixture Discriminant Analysis','Logistic Model Trees','Neural Network','k-Nearest Neighbors')
mod_code<-c('AdaBag', 'treebag', 'bagFDA','logicBag','avNNet','rf','ada','glmboost','LogitBoost','bstTree','svmRadialWeights','hdda','lda','ORFsvm','parRF','glmnet','lssvmPoly','lssvmRadial','amdai','LMT','nnet','knn')
set.seed(10)
# preparing the parallelization
cores <- detectCores()
cl <- makeCluster(cores)
clusterExport(cl, list("trainset", "mod_code",
"train","trytrain","testset"), envir = environment())
registerDoParallel(cl)
system.time(perf<- parSapply(cl, 1:22,
function(x) trytrain(trainset, testset,mod_code[x])))
