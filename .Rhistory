iter
j
j<-iter %%length(data$Y)
j
100%%100
1%%100
100%100
100%%100
N_iter<-1000 # max number of iterations (preferably multiples of data length)
last<-0 #Last change indicator
j<-1
iter<-1
coefs<-c(0,0,0)
for (iter in (1:N_iter)){
j<-iter %%length(data$Y)+1
cat('\niter: ',iter,' coefs: ',as.character(coefs))
if (data$Y[j]!=sign(sum(coefs*data[j,cdata]))){ #classification error
coefs<-coefs + data$Y[j]*data[j,cdata] #Actualize coefficients
last<-j #Actualize indicator of last change
} else {
if (last==j) {
break # break if one complete round without changes
}
}
}
#Plot data and classifiers
ggplot(data = data, aes(x = height, y = weight,
colour=Animal, fill=Animal)) +
geom_point() +
xlab("Height") +
ylab("Weight") +
geom_point(size=5,aes(x=c(mid_point[2]),
y=c(mid_point[1])))+
geom_point(size=5,aes(x=c(catmed[2]),
y=c(catmed[1])))+
geom_point(size=5,aes(x=c(dogmed[2]),
y=c(dogmed[1])))+
geom_abline(aes(slope=-(coefs[2]/coefs[1]),intercept=-(coefs[3]/coefs[2])))
-(coefs[2]/coefs[1])
class(-(coefs[2]/coefs[1]))
ggplot(data = data, aes(x = height, y = weight,
colour=Animal, fill=Animal)) +
geom_point() +
xlab("Height") +
ylab("Weight") +
geom_point(size=5,aes(x=c(mid_point[2]),
y=c(mid_point[1])))+
geom_point(size=5,aes(x=c(catmed[2]),
y=c(catmed[1])))+
geom_point(size=5,aes(x=c(dogmed[2]),
y=c(dogmed[1])))+
geom_abline(aes(slope=as.numeric(-(coefs[2]/coefs[1])),
intercept=as.numeric(-(coefs[3]/coefs[2]))))
coefs
slope<--(coefs[2]/coefs[1])
int<--(coefs[3]/coefs[2])
slope<-as.numeric(-(coefs[2]/coefs[1]))
int<-as.numeric(-(coefs[3]/coefs[2]))
abline(slope,int)
abline(int,slope)
slope
int
plot(data$height,data$weight)
head(data)
plot(1,2)
library(mvtnorm)
library(ggplot2)
library(e1071)
##### Functions #####################
#-------------------------------------
##### Functions to Generate data#############
# create small wrapper functions
# covariance matrix
sigmaXY <- function(rho, sdX, sdY) {
covTerm <- rho * sdX * sdY
VCmatrix <- matrix(c(sdX^2, covTerm, covTerm, sdY^2),
2, 2, byrow = TRUE)
return(VCmatrix)
}
# bivariate normal
genBVN <- function(n = 1, seed = NA, muXY=c(0,1), sigmaXY=diag(2)) {
require(mvtnorm)
if(!is.na(seed)) set.seed(seed)
rdraws <- rmvnorm(n, mean = muXY, sigma = sigmaXY)
return(rdraws)
}
# creating a function to generate bivariate data
catsAndDogs <- function(noCats, noDogs, muCats, muDogs, sdCats,
sdDogs, rhoCats, rhoDogs, seed=1111) {
sigmaCats <- sigmaXY(rho=rhoCats, sdX=sdCats[1], sdY=sdCats[2])
sigmaDogs <- sigmaXY(rho=rhoDogs, sdX=sdDogs[1], sdY=sdDogs[2])
cats <- genBVN(noCats, muCats, sigmaCats, seed = seed)
dogs <- genBVN(noDogs, muDogs, sigmaDogs, seed = seed+1)
animalsDf <- as.data.frame(rbind(cats,dogs))
Animal <- c(rep("Cats", noCats), rep("Dogs", noDogs))
animalsDf <- cbind(animalsDf, Animal)
colnames(animalsDf) <- c("weight", "height", "Animal")
return(animalsDf)
}
# Data generation using previous functions, overlap included
data.gen<-function(number=50,meanc2= c(10, 80),sdc2=c(2,10),overlap=F){
noCats <- number; noDogs <- number
if (overlap==F){
animalsDf <- catsAndDogs(noCats, noDogs, c(3, 150), meanc2,
c(2,10), sdc2, -0.1, 0.6)
} else {
animalsDf <- catsAndDogs(noCats, noDogs, c(5, 130), c(8, 100),
c(2,20), c(2,20), -0.1, 0.6)
}
animalsDf$Y<-as.numeric(animalsDf$Animal == levels(animalsDf$Animal)[1])-
as.numeric(animalsDf$Animal == levels(animalsDf$Animal)[2])
# Cats +1 dogs -1
return(animalsDf)
}
colMedians<-function(data){
medians<-c()
for (i in 1:dim(data)[2]){
medians[i]<-median(data[[i]])
}
return (medians)
}
#' perceptron function
#'
#' Executes bivariate variable perceptron estimates of coefficients to classify a binary class
#'
#' @param data Data frame, first two columns are x2, x1, third column is a two level class, fouth column its +1 -1 equivalent
#' @param N_iter Number of maximum iterations of the perceptron
#' @param verbose Boolean to indicate if detailed info is to be provided during iterations
#' @return list of estimated coefficients and iterations required
#' @export
perceptron<-function(data,N_iter=10000,verbose=F){
coefs<-runif(3)
names(data)<-c("weight","height","class","Y")
# Add dimension for intercept estimation (fixed 1)
data$i<-rep(1,length(data$Y))
cdata<-c(1,2,5)
#initialize loop variables
last<-0 #Last change indicator
j<-1
iter<-1
for (iter in (1:N_iter)){
j<-iter %%length(data$Y)+1
if (verbose==T) {cat('\niter: ',iter,' coefs: ',as.character(coefs))}
if (data$Y[j]!=sign(sum(coefs*data[j,cdata]))){ #classification error
coefs<-coefs + data$Y[j]*data[j,cdata] #Actualize coefficients
last<-j #Actualize indicator of last change
} else {
if (last==j) {
break # break if one complete round without changes
}
}
}
return(list(coefs=coefs,iter=iter))
}
#' plot.perceptron function
#'
#' Executes bivariate variable perceptron estimates of coefficients to classify a binary class
#'
#' @param data Data frame, first two columns are x2, x1, third column is a two level class, fouth column its +1 -1 equivalent
#' @param coefs Estimade coefficients for the perceptron (x2 coef, x1 coef, b coef)
#' @param minpoint Data frame of single point, just as 'data' but adding Distance column with the desired word to name it
#' @param plotmin Boolean. True if needed to plot the minpoint
#' @return plot of data, classification line, and minimum distance point
#' @export
#'
plot.perceptron<-function(data,coefs,minpoint=NULL,plotmin=F){
line<-data.frame(slope=0,int=0)
names(data)[1:3]<-c("weight","height","class")
#calculate slope and intercept
line$slope<-as.numeric(-(coefs[2]/coefs[1]))
line$int<-as.numeric(-(coefs[3]/coefs[1]))
if (plotmin==T){ #plot including min distance classifier
plot<- ggplot() +
geom_point(data = data, aes(x = height, y = weight,
colour=class, fill=class)) +
geom_point(data=minpoint,aes(x=height,y=weight,shape=Distance))+
xlab("X1") +
ylab("X2") +
geom_abline(data=line,aes(slope=slope,
intercept=int))
} else {
#Plot data and classifiers
plot<-ggplot(data = data, aes(x = height, y = weight,
colour=class, fill=class)) +
geom_point() +
xlab("X1") +
ylab("X2") +
geom_abline(data=line,aes(slope=slope,
intercept=int))
}
return(plot)
}
#' classify perceptron function
#'
#' Classifies using bivariate variable perceptron estimates of coefficients to classify a binary class, and computes final distance to threshold.
#'
#' @param data Data frame, first two columns are x2, x1, third column is a two level class, fouth column its +1 -1 equivalent
#' @param coefs Initial coefficients for the perceptron (x2 coef, x1 coef, b coef)
#' @return Data frame with predictions and distances
#' @export
classify.perceptron<-function(data,coefs){
data$i<-rep(1,length(data$Y))
cdata<-c(1,2,5)
pred<-rep(NA,dim(data)[1])
dist<-rep(NA,dim(data)[1])
int<-as.numeric(-(coefs[3]/coefs[1])) #intercept
# Classifiy and distance
for(i in 1:length(data$Y)){
pred[i]<-sign(sum(coefs*data[i,cdata]))
dist[i]<-abs((coefs[1]*(data[i,1]-int)+coefs[2]*data[i,2])
/sqrt(sum(coefs[1:2]^2)))
}
dist<-unlist(dist)
return(data.frame(pred=pred,dist=dist))
}
#' Radius of data function
#'
#' Calculates max radius of bivariatedata as distance to its mean
#'
#' @param data Data frame, first two columns are x2, x1
#' @return Numeric maximum distance from a point to the centroid of data
#' @export
calc.radius<-function(data){
radius<-0
mean<-colMeans(data[,1:2])
for (i in 1:dim(data)[1]){
dist<-sum((mean-data[i,1:2])^2)
if (dist>radius) {radius<-dist}
}
return(sqrt(radius))
}
#' Optimal margin function
#'
#' Calculates optimal margin separation of bivariatedata as distance to linear svm classifier line
#'
#' @param data Data frame, first two columns are x2, x1,class is Y (-1,+1)
#' @return Numeric optimal classification margin
#' @export
calc.gamma<-function(data){
names(data)[1:2]<-c("weight","height")
gamma<-0
svm.model <- svm(Y ~ height+weight, data=data, type='C-classification',
kernel='linear',scale=FALSE)
border<-data[svm.model$index,]
w <- t(svm.model$coefs) %*% svm.model$SV
b <- -svm.model$rho
p <- svm.model$SV
intp<--b/w[1,2]
sl<--w[1,1]/w[1,2]
svm.coefs<-c(1,-sl,-intp)
names(svm.coefs)<-c("weight","height","i")
norm.w<-sqrt(svm.coefs[1]^2+svm.coefs[2]^2)
gamma<-sum(svm.coefs[1:2]*(border[1,1:2]-c(intp,0)))/norm.w
#check wih other point uniqueness
# gamma<-sum(svm.coefs[1:2]*border[2,1:2])/norm.w
return(as.numeric(abs(gamma)))
}
#-------------------------
#### Algorithm ###########
#-------------------------
#Data generation case separation
data<-data.gen(number=100,overlap=F)
coefs<-perceptron(data,50000)$coefs
plot<-plot.perceptron(data,coefs)
plot
out<-classify.perceptron(data,coefs)
head(out)
# Plot max margin solution with linear support vector model
svm.model <- svm(Y ~ height+weight, data=data, type='C-classification',
kernel='linear',scale=FALSE)
border<-data[svm.model$index,]
border$Distance<-c("svm")
w <- t(svm.model$coefs) %*% svm.model$SV
b <- -svm.model$rho
p <- svm.model$SV
intp<--b/w[1,2]
sl<--w[1,1]/w[1,2]
svm.coefs<-c(1,-sl,-intp)
# plot perceptron and linear svm lines
plot+geom_abline(slope=sl,intercept=intp,colour="red")
#Errors
Errors<-(length(data$Y)-sum(out$pred*data$Y))/2
Errors/lenght(data$Y)#percent errors
# Plot with min dist point
minpoint<-data[which.min(out$dist),]
minpoint$Distance<-c('minimum')
plot.perceptron(data,coefs,minpoint,plotmin=T)
#### Case overlap data
data<-data.gen(number=100,overlap=T)
coefs<-perceptron(data,20000)$coefs
plot<-plot.perceptron(data,coefs)
plot
out<-classify.perceptron(data,coefs)
#Errors
Errors<-(length(data$Y)-sum(out$pred*data$Y))/2
Errors/length(data$Y) #percent errors
# Plot with min dist point
minpoint<-data[which.min(out$dist),]
minpoint$Distance<-c('minimum')
plot.perceptron(data,coefs,minpoint,plotmin=T)
# Check efect of radius and minimum distance on number of iterations
# Generate random data and check
data<-data.gen(number=100,overlap=F)
#including points in sequence along centers of data,
# affects distance and radius at same time
c1center<-c(3, 150)
c2center<-c(c(10, 80))
mean<-colMeans(data[,1:2])
slpct<-(c1center[1]-c2center[1])/(c1center[2]-c2center[2])
intct<-c1center[1]-c1center[2]*slpct
#seq of points from center of class 2 of data to center of data,
#generate each time closer points
n_each<-5
intpoint<-seq(80,mean[2],length.out=n_each)
intpoint<-as.data.frame(cbind(intpoint*slpct+intct,intpoint))
intpoint<-intpoint[1:(n_each-1),] #remove point at mean
names(intpoint)<-c("weight","height")
# idem but outer
outpoint<- seq(0,80,length.out=n_each)
outpoint<-as.data.frame(cbind(outpoint*slpct+intct,outpoint))
outpoint<-outpoint[1:(n_each-1),] #remove last point repeated
names(outpoint)<-c("weight","height")
#join
points<-rbind(outpoint,intpoint)
#plot to check it
ggplot() +
geom_point(data = data, aes(x = height, y = weight,
colour=Animal, fill=Animal)) +
geom_point(data=points,aes(x=height,y=weight))
# for each new center of class 1, play with increasing or decreasin SD in order
# to generate data with diferent margin of classification and radius
n_sd<-5
n_xclass<-100
max_iter<-30000
sd<-data.frame(sdx2=seq(1,6,length.out=n_sd),sdx1=seq(4,20,length.out=n_sd))
results<-matrix(NA,dim(points)[1]*dim(sd)[1],ncol=5)
for(i in (1:dim(points)[1])){
for (j in (1:dim(sd)[1])){
cat('\n Point ',i,' SD ',j)
data <- data.gen(n_xclass,meanc2= as.numeric(points[i,]),
sdc2=as.numeric(sd[j,]),overlap=F)
percp<-perceptron(data,max_iter)
# Radius and gamma calculations
gamma<-calc.gamma(data)
radius<-calc.radius(data)
results[j+(i-1)*dim(sd)[1],]<-c(i,j,radius,gamma,percp$iter)
}
}
# Final plot
results<-as.data.frame(results)
names(results)<-c("center2","sd2","radius","gamma","iterations")
results<-results[results$iterations<max_iter,]
results$rad_dist<-(results$radius/results$gamma)^2
ggplot(results, aes(x=radius, y=iterations)) +
geom_point(shape=1) +    # Use hollow circles
geom_smooth(method=lm)+
ggtitle("Performance of percerptron algorithm")
ggplot(results, aes(x=gamma, y=iterations)) +
geom_point(shape=1) +    # Use hollow circles
geom_smooth(method=lm)+
ggtitle("Performance of percerptron algorithm")
ggplot(results, aes(x=rad_dist, y=iterations)) +
geom_point(shape=1) +    # Use hollow circles
geom_smooth(method=lm)+
xlab(expression(paste((radius/gamma)^bold("2"))))+
ggtitle("Performance of percerptron algorithm")
data<-data.frame(rnorm(10),rnorm(10))
calc.radius(data)
plot(data)
plot(data,asp=1)
mean<-colMeans(data[,1:2])
mean
results
library(tree)
library(tree)
install.packages("tree")
install.packages("rpart")
?rpart
library(rpart)
library(tree)
ir.tr <- tree(Species ~., iris)
ir.tr
summary(ir.tr)
plot(ir.tr)
fit <- tree(Species ~., iris)
fit
summary(fit)
plot(fit, uniform=TRUE, main="Classification Tree")
text(fit, use.n=TRUE, all=TRUE, cex=.8)
plot(fit, main="Classification Tree")
text(fit,  all=TRUE, cex=.8)
str(iris)
data<-iris
fit2 <- rpart(data$Species ~ ., method="class")
data<-iris
fit2 <- rpart(Species ~ Sepal.Length+Sepal.Width+Petal.Length+Petal.Width,
data=data, method="class")
plot(fit, uniform=TRUE, main="Classification Tree")
text(fit, use.n=TRUE, all=TRUE, cex=.8)
rpart.control(maxdepth=1)
fit2 <- rpart(Species ~ Sepal.Length+Sepal.Width+Petal.Length+Petal.Width,
data=data, method="class")
plot(fit, uniform=TRUE, main="Classification Tree")
text(fit, use.n=TRUE, all=TRUE, cex=.8)
control<-rpart.control(maxdepth=1)
fit2 <- rpart(Species ~ Sepal.Length+Sepal.Width+Petal.Length+Petal.Width,
data=data, control=control,method="class")
plot(fit, uniform=TRUE, main="Classification Tree")
text(fit, use.n=TRUE, all=TRUE, cex=.8)
control<-rpart.control(maxdepth=1)
fit2 <- rpart(Species ~ Sepal.Length+Sepal.Width+Petal.Length+Petal.Width,
data=data, control=rpart.control(maxdepth=1),
params=list(split="gini"),method="class")
plot(fit2, uniform=TRUE, main="Classification Tree")
text(fit2, use.n=TRUE, all=TRUE, cex=.8)
fit2 <- rpart(Species ~ Sepal.Length+Sepal.Width+Petal.Length+Petal.Width,
data=data, control=control,
method="class")
plot(fit2, uniform=TRUE, main="Classification Tree")
text(fit2, use.n=TRUE, all=TRUE, cex=.8)
str(data)
out<-predict(fit2,data[,-5])
head(out)
head(out,50)
tail(out,50)
tail(out,100)
out<-predict(fit2,data[,-5],type="class")
tail(out,100)
ntrees<-c(1:(dim(data)[1]))
devtools::install_github("rstudio/rmarkdown")
rmarkdown::render('in.md',
output_format=pdf_document(latex_engine='xelatex')
)
if(!require("randomForest"))install.packages("randomForest")
if(!require("doParallel"))install.packages("doParallel")
if(!require("reshape2"))install.packages("reshape2")
if(!require("ggplot2"))install.packages("ggplot2")
if(!require("caret"))install.packages("caret")
if(!require("dplyr"))install.packages("dplyr")
if(!require("ggthemes"))install.packages("ggthemes")
setwd("D:/master/kaggle/TheSupportVectors")
source("code/library.R")
# Function for random forest
compare.k <- function(k, train, label) {
#compare errors for different k maximum variables on features
model <- randomForest(x = train, y = label, mtry = k)
# error rate is computed from out of bag elements, so it's a cross-val error
rfor.error<-sum(model$predicted!=label)/length(label)
return(model$err.rate[dim(model$err.rate)[1],])
}
#get data for rforest
train.data <- get.train.data()
labels <- as.factor(train.data[[1]])
features <- train.data[[3]] #standarized data
features[,10:53]<-as.data.frame(apply(features[,10:53],2,as.factor))
#subset data
features<-features[1:5000,]
labels<-labels[1:5000]
source("code/library.R")
source("code/library.R")
train.data <- get.train.data()
labels <- as.factor(train.data[[1]])
features <- train.data[[3]] #standarized data
features[,10:53]<-as.data.frame(apply(features[,10:53],2,as.factor))
features<-features[1:5000,]
labels<-labels[1:5000]
#one model
model <- randomForest(x = features, y = labels, mtry = 12)
save(model,"model.RData")
labels2<-labels==1
class(labels)
head(labels)
head(labels2)
labels2<-1*(labels==1)
head(labels2)
labels2<-as.factor(1*(labels==1))
head(labels2)
models<-list()
probs<-loadRDS(probs,"rforest_probs.rds")
probs<-readRDS(probs,"rforest_probs.rds")
probs<-readRDS("rforest_probs.rds")
probs<-readRDS("data/rforest_probs.rds")
head(probs[[1]])
probs_class<-matrix(NA,dim(features)[1],length(levels(labels)))
for (i in 1:7){
probs_class[,i]<-probs[[i]][,i]
}
probs[[i]][,2]
probs_class<-matrix(NA,dim(features)[1],length(levels(labels)))
for (i in 1:7){
probs_class[,i]<-probs[[i]][,2]
}
head(probs_class)
test.data <- get.test.data()
test.feat <- test.data[[2]]
test.feat[,10:53]<-as.data.frame(apply(test.feat[,10:53],2,as.factor))
probs_class<-matrix(NA,dim(test.feat)[1],length(levels(labels)))
for (i in 1:7){
probs_class[,i]<-probs[[i]][,2]
}
p<-matrix(1:30,3,10)
p
sapply(p,1,which.max)
apply(p,1,which.max)
p[3,2]<-55
apply(p,1,which.max)
predict<-apply(probs_class,1,which.max)
head(predict)
head(probs_class)
id <- read.csv(file = "data/Kaggle_Covertype_test_id.csv", header = T)[,1]
prediction <- data.frame(id =id, Cover_Type = predict)
write.csv(x = prediction, file = "data/rfor/rfor_test_prediction_agg.csv", row.names = FALSE)
